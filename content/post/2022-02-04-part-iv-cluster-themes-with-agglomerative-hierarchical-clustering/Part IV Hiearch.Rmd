---
title: Part IV -- Find Similar Themes with Agglomerative Hierarchical Clustering
author: Rees W. Morrison
date: '2022-02-04'
categories:
  - Analysis
tags:
  - Agglomerative Hierarchical Clustering 
output: pdf_document
draft: no
---

<!-- 10:13:24 AM: Build failed due to a user error: Build script returned non-zero exit code: 2 -->

```{r packages, echo=F, warning=FALSE, message=FALSE, include=FALSE, results='hide'}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(tidyverse)  
library(ggthemes) 
library(ggrepel) 
library(readxl) 
library(GGally)
library(tidytext) 
library(blogdown)
library(rainette)  # had problem installing faststack
library(cluster) # clustering algorithms and gap statistic
library(factoextra) # distance matrix and plot k-means clusters
library(kableExtra)

```


```{r means1, eval=TRUE, warning=FALSE, echo=FALSE}

comboData <- read_xlsx(path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Analytics/comboData.xlsx")

# convert Theme to factor - why?  to try to add labels to corr matrix
comboData$Theme <- as.factor(comboData$Theme)
# comboData$Theme <- as.character(comboData$Theme)

comboDataMtx <- as.matrix(comboData)  # all numeric, row names 1 to 20

```

```{r normalize1}
comboScaled <- scale(x = comboData[ , 2:20], center = TRUE, scale = TRUE)

comboScaledDF <- as.data.frame(scale(x = comboData[ , 2:20]))
comboScaledDF$Metric <- comboData[  , 1]

```

[Part I](part1) of this working series introduced Themes, and [Part II](part II) introduced certain metrics pertaining to those Themes. [Part III](part III) started the analysis of Theme centrality and complexity by an unsupervised clustering algorithm, k-means clustering.  It requires the analyst to specify the number of clusters, and figuring out the optimal number of clusters can often be hard. Part III side-stepped that challenge because the k-means algorithm produces data that allows you to calculate the Euclidean distance between closest, most-similar Themes. In this post we introduce a second algorithm for the same task. **Agglomerative hierarchical clustering** is an alternative unsupervised approach; it builds a hierarchy of closest Themes and in doing so creates data that reflects closeness of Themes.  That orderly construction of clustering helps identify most-similar Themes, referred to as "closest pairs."

The hierarchical algorithm starts by putting each Theme in its own unit.  It then identifies the closest other Theme by the Euclidean distance between them in a **hyperspace** of 20 dimensions (each Theme is point in that space as determined by the vector of its metrics). At first, a single Theme is clumped with another single Theme, if they are closest to each other, each clumping thereby creating a two-Theme unit as a combined point in hyperspace.  Step-by-step the algorithm agglomerates each remaining Theme (or unit) to its closest Theme or unit (based on the chosen **link method**, where we used average linkage); if an already-agglomerated cluster is closest, it clusters the nearest Theme with that unit.  Eventually, all the Themes end up in a single cluster.  

Once the clustering ends, the results are usually visualized by a **dendrogram**.  On the dendrogram, you can tell that two Themes are most similar because they are close on the x axis and the height of the link that joins them together is short.  That height on the y-axis is the value of the distance metric between the two Theme points in hyperspace.  

In a dendrogram, which subtree should go on the left and which on the right follows the rule that the tighter cluster is on the left (the last, i.e., most recent, merge of the left subtree is at a lower value than the last merge of the right subtree). Merges involving two Themes place them in order by their observation sequence number, which in this instance is the order in which the Themes were first posted on Themes from Art.

```{r hierarchicalPlot2, warning=FALSE, echo=FALSE, message=FALSE}
# Finding distance matrix; what is p = power of the Minkowski distance
distance_mat <- dist(comboData, method = 'euclidean')

# labels and numbering are by observation sequence order

set.seed(240)  # Set seed for reproducibility   ALSO COULD USE Agnes package
Hierar_cl <- hclust(distance_mat, method = "average")  # average = centroid of the cluster
# negative entries in merge indicate agglomerations of singletons, and positive entries indicate agglomerations of non-singletons.

Hierar_cl$labels <- c("alcohol", "chance", "beauty", "silence", "work", "decisions", "destruction", "time", "death", "trains", "church", "birds", "friends", "sailing", "sports","bridges",  "soldiers", "wind","clothes",  "dancing", "sleep", "rivers", "money", "night")

# Must use "seed" to stay consistent.  Interpretation: plot(Hierar_cl) without labels numbers the leafs. The merge element of the list [1] shows from the bottom up which mergers happened, in what order.  

# create a data frame of merger order, obs, height; get abs value of obs to left_join
HierDF <- data.frame(Height = Hierar_cl[2], ObsSeq = Hierar_cl[1])
colnames(HierDF) <- c("Height", "ObsSeq1", "ObsSeq2")
HierDF$Abs1 <- abs(HierDF$ObsSeq1)
HierDF$Abs2 <- abs(HierDF$ObsSeq2)

ThemeLookup <- read_xlsx("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "NoSThemes", range = "B1:D25")
colnames(ThemeLookup) <- c("Theme", "Chrono", "Alpha")

HierDF <- left_join(x = HierDF, y = ThemeLookup[  , c(1, 2)], by = c("Abs1" = "Chrono"))
HierDF <- left_join(x = HierDF, y = ThemeLookup[  , c(1, 2)], by = c("Abs2" = "Chrono"))
HierDF$Order <- seq(1:nrow(HierDF)) 
HierDF <- HierDF[  , c(1, 2, 3, 6, 7, 8, 4, 5)]  # to make it easier to read below

# writexl::write_xlsx(x = HierDF, path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Analytics/HierDF.xlsx")

# Plotting dendrogram
# plot(Hierar_cl, label=comboData$Theme, hang = -1, xlab = NULL, ylab = NULL)  # how can I trust the labels? did by hand above

```

Let's use R's ggdendro package to visualize the hierarchical clustering results.  So, for example, Alcohol and Silence (second and third from the left on the horizontal) are combined to form the first unit.  The next-to-last agglomeration, because the vertical distance on the left axis is so large, joins Sports with Work and Time (far right).  You can see that Death and Churches are very close, so they cluster very low on the Y axis, and then Trains agglomerates into them.\

```{r hierarchicalPlotgg}
library(ggdendro)

ggdendrogram(Hierar_cl, rotate = FALSE, size = 2, labels = TRUE) + 
  labs(x = "Distance", title = "         Cluster Dendrogram of Themes")

#convert cluster object to use with ggplot
dendr <- ggdendro::dendro_data(Hierar_cl, type="rectangle") 
# dendr[1]  segments with x, y, xend and yend
# dendr[2]  labels with x, y, label (which is a number, not sure alpha or order I created)

#your own labels are supplied in geom_text() and label=label, from comboData$Theme
# https://stackoverflow.com/questions/21474388/colorize-clusters-in-dendogram-with-ggplot2
DendPlot <- ggplot() + 
  geom_segment(data=segment(dendr), aes(x=x, y=y, xend=xend, yend=yend)) + 
  geom_text(data=label(dendr), aes(x=x, y=y, label=comboData$Theme, hjust=0), size=3) +
  coord_flip() + 
  scale_y_reverse(expand=c(0.2, 0)) + 
  labs(x = NULL, y = NULL, title = "Dendrogram of Themes", subtitle = "Hierarchical agglomerative clustering of metrics") +
  theme_tufte(ticks = FALSE)

# DendPlot

```
