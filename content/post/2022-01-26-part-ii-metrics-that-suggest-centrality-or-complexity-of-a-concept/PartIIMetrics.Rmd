---
title: "Part II -- Metrics Suggesting Centrality or Complexity of Themes"
author: "Rees W. Morrison"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
categories: Analysis
tags: Metrics
draft: yes
slug: []
---

```{r metricslist, echo=F, warning=FALSE, message=FALSE, include=FALSE, results='hide'}

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE)

library(tidyverse)  
library(ggthemes) 
library(ggrepel) 
library(readxl) 
library(GGally)
library(tidytext) 

library(blogdown)
library(colorBlindness)
library(patchwork)
 
library(rainette)  # had problem installing faststack
library(cluster) # clustering algorithms and gap statistic
library(factoextra) # distance matrix and plot k-means clusters

library(kableExtra)

```

[Part I](partI) of this series of five posts (working notes for myself in fact) provides an overview of the Themes analyzed in this introductory investigation of centrality and complexity.  This second post lays out the metrics that underlie the analysis. I present all 20 of them, with the brief introduction of each metric covering similar topics and order: first, why I gathered the metric, next the source of the metric, and then any calculations that were carried out.  Finally, a two-by-two layout of plots shows the results for the four preceding metrics, each of whom loosely shares a common attribute. 

<!--more-->

The table below lists each metric, its source and its (preliminary) classification regarding whether it suggests centrality, complexity or a mixture between the two.  The right-most column shows the attribute by which I sorted the 20 metrics into five groups.  The following pages present the metrics in attribute groups of four.

```{r metricstable}

MetricsList <- read_excel("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "MetricsSummary", range = "A1:D21")

knitr::kable(MetricsList, caption = '20 Metrics', align = "l",
             col.names = c("Metric", "Source(s)", "Class", "Attribute"), booktabs = TRUE) %>% 
      kable_styling(latex_options = "HOLD_position")

# https://stackoverflow.com/questions/52944533/wrap-text-in-knitrkable-table-cell-using-n  could get to work

# knit_table <- function(df){
#   if (is_html_output()) {
#     df %>%
#       kable("html", escape = F) %>%
#       kable_styling()
#   } else {
#     df <- data.frame(lapply(df, function(x) {gsub("<br>", "\n", x)}), stringsAsFactors = F)
# 
#     df %>%  
#       mutate_all(linebreak) %>%
#       kable("latex", booktabs = T, escape = F)  
#   }
# }
# knit_table(MetricsList, caption = '20 Metrics', align = "l",
#              col.names = c("Metric", "Source(s)", "Class", "Sort"), booktabs = TRUE,
#              col1 = MetricsList$Metric,
#              col2 = MetricsList$Source,
#              col3 = MetricsList$Suggests,
#              col4 = MetricsList$Order) %>%
#     kable_styling(latex_options = "HOLD_position")

```

## Synonyms

One attribute of a Theme is the number of words in English that are deemed to have a similar meaning.  The more synonyms that exist, therefore, the more people have used the idea and developed nuances; thus both centrality and complexity could correlate to the number of synonyms available.

For each Theme, we found the number of synonyms that a leading dictionary gives for the concept. The source, [Thesaurus.com](https://thesaurus.com), identifies "most relevant" synonyms (colored red on the website) and less-close synonyms (colored orange).  The metric for this analysis consists of the sum of both kinds of synonyms for each Theme.

<!-- A naive hypothesis might maintain that concepts (i.e., words) that have more synonyms represent more nuanced ideas, because various words exist that denote or connote close but different meanings or shades of meanings. Not use antonyms; not use double source (OED?) and add or average -->

The plot in the quartet plot below depicts for each Theme the aggregated number of its synonyms.\

```{r syns, message=FALSE, warning=FALSE}
syns <- read_excel("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "SynAntonym", range = "A1:B25")
```

```{r synsPlot, eval=TRUE}
PlotSyn <- ggplot(syns, aes(x = reorder(Theme, Synonyms), y = Synonyms)) +
  geom_point(size = 3, color = colorNames()[1]) +
  labs(x = NULL, y = "Total Synonyms", title = "Synonyms", caption = "\nSource: Thesaurus.com") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0)) +
  coord_flip()

# PlotSyn

```

## Oxford English Dictionary (OED), Frequency Bands

As with Google Trillion words, frequency of use could correspond to centrality as well as complexity.  Centrality because it is much discussed and written about, and complexity because relatively many ideas, implications, extensions, comparisons, and applications come to mind about the concept. 

The Oxford English Dictionary (OED) assigns each of its words a band from 1 to 8 according to how frequently the OED has determined that the term appears per million words. Specifically, Frequency Band 6 contains words that occur between 10 and 100 times per million words in typical modern English usage.    Frequency Band 7 contains words that occur between 100 and 1,000 times per million words (i.e., at least ten times more frequently than Band 6 words appear), while Frequency Band 8 covers words that occur more than 1,000 times per million words. This highest band includes the most common English words, such as determiners (e.g., the, a, an, this, that), pronouns, principal prepositions, and conjunctions (e.g.,  and, but, that, if).

For each Theme, the metric is its OED Frequency Band.  We also compiled the earliest year the OED has found of written use of the Theme term, and plotted both year and band.\

```{r oed}
oed <- read_excel("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "OED", range = "A1:C25")
```

```{r oedPlot, eval=TRUE}
PlotOED <- ggplot(oed, aes(x = reorder(FirstUse, FirstUse), y = OEDBand)) +
  geom_point(size = 3, color = "SkyBlue") +
  geom_text_repel(aes(label = Theme), size = 2.7, hjust = .5, direction="y") +
  labs(x = "Earliest Written Use", y = "Frequency Band", title = "Oxford English Dictionary Frequency Bands", caption = "Source: Princeton Pub. Lib. access to OED Online\n https://www.oed.com/") +
  theme_tufte(ticks = FALSE) +
  coord_flip()

# PlotOED
```

## 1,000 Most Common English Words, Rank

The same reasoning that led to metrics from Google's Trillion Word data base and the OED frequency bands supports the inclusion of rankings of Themes among the thousand most common words in English. I combined three sources to create the list used here.  One is [1000 Most Common Words](https://1000mostcommonwords.com/1000-most-common-english-words/), a second was [Wikipedia](https://en.wikipedia.org/wiki/Most_common_words_in_English), and the third came from [Smart Words](https://www.smart-words.org/500-most-commonly-used-english-words.html).  

If the Theme was expressed as a plural, such as "Soldiers", I determined the ranking of the singular form.  Any Theme that was not included in the composite list received a ranking of 1001. For each Theme, the plot below shows its rank on the compiled list.\

<!-- discuss truncation at top and bottom -->

```{r fivehundred}
# data from /bettRwritR directory, Thousand most common words in English.xlsx
top <- read_excel("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "Top1000", range = "A1:B25")
```

```{r fivehundredPlot, eval=TRUE}
PlotTop <- ggplot(top, aes(x = reorder(Theme, Top1000), y = Top1000)) +
  geom_point(size = 3, color = "#F0E442") +   # bluishGreen
  labs(x = NULL, y = "Rank", title = "Rank among 1,000 Most Common Words", caption = "\nSources: https://1000mostcommonwords.com/\nhttps://en.wikipedia.org/wiki/Most_common_words_in_English\nhttps://www.smart-words.org/500-most-commonly-used-english-words.html") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0)) +
  coord_flip()

# PlotTop
```

## Google NGram Viewer, Percentage of Book References

What people write about in books gives insight into what is important in their thinking.  For Themes, therefore, the metric suggests complexity more than centrality.

The [Google Ngram Viewer](https://books.google.com/ngrams) charts the frequencies of search strings using a yearly count of n-grams found in sources printed between 1500 and 2019 in Google's text corpora.  I searched for each Theme and added together the percentages given for lower case, initial caps, and all caps of the word for the latest year, 2000.  


```{r viewer}
viewer <- read_excel("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "GViewer")
viewer <- viewer[1:24, 1:2]  # drop think and extra source column
```

```{r viewerPlot, eval=TRUE}
PlotNgram <- ggplot(viewer, aes(x = reorder(Theme, Gbooks), y = Gbooks)) +
  geom_point(size = 3, color = "#D55E00") +   # vermillion of colorBlindness
  labs(x = NULL, y = "Google Viewer Pcts.", title = "Google Viewer Percents", caption = "\nSource: https://books.google.com/ngrams/") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0))+
  scale_y_continuous(labels = scales::comma) +
  coord_flip()

# PlotNgram
```

Here are the four preceding plots, whose metrics share an attribute of Theme frequency.

```{r frequency, fig.width = 8.5, fig.height = 9}

patchFrequency <- wrap_plots(PlotSyn & theme_minimal(), PlotOED & theme_minimal(), PlotTop & theme_minimal(), PlotNgram & theme_minimal(), ncol = 2)

patchFrequency

```


## Google's Trillion-Word Database, Frequency

A metric of how common a Theme is on the internet could point us toward its centrality, because it is foremost in the minds of many writers, or perhaps to its complexity, because more has been written about the concept than about a less-frequently used concept. 

One source of data about the frequency with which words have been used comes from the database created by Google's [Peter Norvig](https://norvig.com/ngrams/count_1w.txt). In 2006, Google made available 1,024,908,267,229 words (one trillion, 24 billion words) and counts for words that appeared at least 40 times. The metric used here simply searched for each Theme word and included the count. 

The quartet plot after the next three metrics displays the count for each Theme word.\

<!-- discuss rank vs continuous numbers -->

```{r norvig, message=FALSE, echo=FALSE}
norvig <- read_excel("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "NorvigCounts")
norvig <- norvig[1:24, c("Theme", "Trillion")]  # dropping Thinking
```

```{r norvigPlot, eval=TRUE}
PlotNorvig <- ggplot(norvig, aes(x = reorder(Theme, Trillion), y = Trillion)) +
  geom_point(size = 3, color = colorNames()[2]) +
  labs(x = NULL, y = "Total Counts on Google's Trillion Word Corpus from the Web", title = "Google Trillion Word Counts", caption = "Source: https://norvig.com/ngrams/") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0)) +
  scale_y_continuous(breaks = seq(0, 2e9, 300e6), labels = scales::comma) +  
  coord_flip()

# PlotNorvig
```

## Google Search Results, Number Returned

Another hypothesis for the relative centrality of a Theme holds that the more people have published a term on the internet, the more relevant it is to their lives: the more central.  Thus, the number of hits for a Theme found by a powerful search engine crudely reflects its relative currency and salience.

For this metric, I entered a search into Google for each Theme.  When the results page returned, I recorded the number of results from the top left of my screen (e.g., for Thinking it said "About 992,000,000 results".  A later post will incorporate comparable search data results from Bing.

<!-- may need to keep searching: "alcohol" increased from 749 million to just over a billion! And what about drop off -->

The data collected appear in the plot below, where each Theme sits to the left of the point that shows the Google results hits.\

```{r google, message=FALSE, echo=FALSE}
google <- read_excel("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "GoogleBingHits", range = "A1:B25")  # left Bing off
```

```{r googlePlot, eval=TRUE}
PlotSearch <- ggplot(google, aes(x = reorder(Theme, GoogleResults), y = GoogleResults)) +
  geom_point(size = 3, fill = "black", color = "grey30") +
  labs(x = NULL, y = "Search Results on Google", title = "Google Search Results", caption = "\nSource: Firefox browser and Google search") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0, size = 8)) + 
  scale_y_continuous(labels = scales::comma) +
  coord_flip()

# PlotSearch

```

## Google Trends, Internet Search Requests

[Google Trends](https://support.google.com/trends/answer/4365533?hl=en&ref_topic=6248052) provides access to a largely unfiltered sample of search requests made using Google. To make comparisons between terms easier Google Trends normalizes the search results to the time and location of a query by the following process:

Each data point is divided by the total searches of the geography and time range it represents to compare relative popularity. The resulting numbers are then scaled from 0 to 100 based on a topicâ€™s proportion to all searches on all topics.

Numbers represent search interest relative to the highest point on the chart for the United States and the year. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means there was not enough data for the term.

The metrics are plotted in the corresponding chart below.\

```{r trends}
trends <- read_excel("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "GoogleTrends", range = "A1:B25")
```

```{r trendsPlot, eval=TRUE}
PlotGTrend <- ggplot(trends, aes(x = reorder(Theme, GSearches), y = GSearches)) +
  geom_point(size = 3, pch = 5, color = "#CC79A7") + # colorBlindness reddishPurple
  labs(x = NULL, y = "Trend Average", title = "Google Search Trends", caption = "\nSource: https://trends.google.com/trends/?geo=US") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0)) +
  scale_y_continuous(labels = scales::comma) +  # remove scientific notation
  coord_flip()

# PlotGTrend

```

## SEO Hashtags

Those who maintain an online presence want their site or advertisement to attract viewers and persuade them to read or to click and buy.  Thus, among many Search Engine Optimization (SEO) techniques, online publishers choose hashtags and keywords to capture terms that will lure searches by search engines to display their company high in the results.  SEO is the effort to game this interchange.  Thus, a Theme term in prominent use online in the form of hashtags rests on centrality: what people want to buy, not anything heady like complexity.

One of the online services that advises marketers regarding search terms, [WordTracker](https://www.wordtracker.com/), provides two sets of data for each Theme word search: *Volume* ("the number of times a search for each keyword appears in our database") and *IAAT* ("The raw number of pages that have the keyword both in the title tag and also in anchor text in a backlink from an external domain."). Because the correlation between these to numbers is only 0.500, I added them together.

```{r tracker, eval=TRUE}
tracker <- read_xlsx(path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "WordTracker", range = "A1:E25")
tracker <- tracker[  , c("Theme", "VolIAAT")]
# cor(tracker$Volume, tracker$IAAT)

```

The lower right plot in the quartet presents the data.\

```{r poemsPlotTwo, eval=TRUE}
PlotTracker <- ggplot(tracker, aes(x = reorder(Theme, VolIAAT), y = VolIAAT)) +
  geom_point(shape = 21, size = 4, fill = "plum3") +
  guides(fill = "none") +
  labs(x = NULL, y = "Hashtag Data Total", title = "Volume plus IAAT from WordTracker", caption = "\nSource: https://www.wordtracker.com/") +
  theme_tufte(ticks = FALSE) +
  scale_y_continuous(labels = scales::comma) +
  coord_flip()

# PlotTracker
```

Here are the four preceding plots, whose metrics share an attribute of a Theme's presence on the internet.

```{r internet, fig.width = 8, fig.height = 9}

patchInternet <- wrap_plots(PlotNorvig & theme_minimal(), PlotSearch & theme_minimal(), PlotGTrend & theme_minimal(), PlotTracker & theme_minimal(), ncol = 2)

patchInternet

```


## Quotations, Number of Appearances

People say something quotable (or are credited with the bon mot) because they are witty and pithy, or because they deliberately craft something memorable in a speech or writing.  Quotations may derive from deep insights (complexity) or from a humorous twist on an everyday topic (centrality), but I lean toward the number of quotations using a Theme as an indicator more of complexity.

The host of [The Quotations Page](http://www.quotationspage.com/) has compiled seven sources of quotations. In the Fall of 2021, the home page claimed 28,000 quotations in English from more than 3,400 authors. Using the site's search engine, I recorded how many quotes have each Theme term.

Additionally, a [Kaggle dataset](https://www.kaggle.com/akmittal/quotes-dataset) includes 22 MB of JSON formatted quotations.  From its 36,937 unique quotes, I recorded how many have each Theme term. Adding the two results together for each Theme generated this metric.

<!-- Discuss correlated 0.997\ -->

```{r quotes}
quotes <- read_excel("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "Quotes", range = "A1:G25")
quotes <- quotes[  , c("Theme", "TotalQuotes")]

# proceed through the entire list and extract the quotes
# quotes.DF <- data.frame(Quotes = sapply(quotes, function(x) x[[1]]))

# quotesUnique <- unique(quotes.DF$Quotes)  # 36937, as original list has many duplicates
# length(which(str_detect(string = quotesUnique, pattern = "think")))
# read in the .txt and remove all but "Quote" line

# quoteKag <- stringi::stri_read_lines(con = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/quotesKaggle.txt", encoding = "UTF8") %>% 
#   paste0(., collapse = " ") %>% 
#   str_remove_all(., pattern = '"')  %>% 
#   str_extract_all(., pattern = "Quote.*Author") %>%  
#   data.frame(Text = ., Post = "Mr. Flood's Party", Theme = "Alcohol")

```

The plot in the combination graphic hereafter displays the total of the counts from the two sources.\  

```{r quotesPlot, eval=TRUE}
PlotQuotes <- ggplot(quotes, aes(x = reorder(Theme, TotalQuotes), y = TotalQuotes)) +
  geom_point(size = 3, pch = 5, color = "chocolate") +
  labs(x = NULL, y = "Aggregated Search Results", title = "Quotations", caption = "Sources: http://www.quotationspage.com/\nhttps://www.kaggle.com/akmittal/quotes-dataset") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0)) +
  coord_flip()

# PlotQuotes

```

## Poem Titles, Count of Instances

By their nature of compression, allusion, meter and rhyme, poems suggest that if a Theme appears in the title, it more likely than not pertains to a deliberate, thoughtful aspect of the poem: ergo, an indicator of the Theme's complexity.

Three online databases of poems provided the data for this metric: [Poetry.com](https://www.poetry.com/psearch/), [Poetry Foundation](https://www.poetryfoundation.org/), and [Poetry.org](http://poetry.org/). I added together the results returned by searches on them for each Theme.

The plot that follows shows a point for each Theme corresponding to the number of combined results.\

<!-- Foundation truncates at 1,000  and includes all text in poem, plus articles etc. Explain or discuss combinations of sources, yet Bing issue.  Suggests correlations-->

```{r poems}
poems <- read_xlsx(path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "Poems", range = "A1:B25")
```

```{r poemsPlot, eval=TRUE}
PlotPoems <- ggplot(poems, aes(x = reorder(Theme, PoemTotal), y = PoemTotal)) +
  geom_point(shape = 21, size = 3, fill = "forestgreen") +
  guides(fill = "none") +
  labs(x = NULL, y = "Total hits", title = "Poem Titals", subtitle = NULL, caption = "\nSources: https://www.poetry.com/psearch/\nhttps://www.poetryfoundation.org//nhttp://poetry.org/") +
  theme_tufte(ticks = FALSE) +
  scale_y_continuous(labels = scales::comma) +
  coord_flip()

# PlotPoems

```

## Figurative Expressions, Count

```{r expressions}

express <- read_xlsx(path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "Expressions", range = "A1:B25") 

```

<!-- All the expression sources, compiled in WorkAddl.Rmd are in the metrics/Expressions -->

Theme terms appear in figurative, idiomatic expressions.  For an example with the Theme of Clothes, "hung out to dry" refers literally to drying clothes on a line, but the expression conveys metaphorically  abandoning someone in a fraught situation.  For each Theme I compiled idiomatic expressions that use the term or an associated term (such as "shirt" for Clothes, or "barn burner" for Fire).  The non-literal phrases came from more than 38 online sources. Expressions become commonplace because they capture an idea that many people adopted as a way of speaking indirectly -- they point toward centrality.

Counting the number of unique expressions (combining slight variations on them) resulted in the `r sum(express$Sayings)`shown in the graphic below as the total per Theme.

<!-- Subjectivity and duplication of expressions, not proverbs or quotes -->

```{r expressionsPlot, eval=TRUE}
PlotExpress <- ggplot(express, aes(x = reorder(Theme, Sayings), y = Sayings)) +
  geom_point(aes(fill = "black"), shape = 21, size = 3, color = "aquamarine4") +
  guides(fill = "none") +
  labs(x = NULL, y = "Number of Figurative Phrases", title = "Figurative Expressions", caption = "\nSource: 38+ online compilations") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0)) +
  coord_flip()

# PlotExpress

```

## Readability of Posts, School Grade Level

It seemed plausible to me that how people write about Themes might match to some degree the centrality or the complexity of the Theme.  Thus, I searched for algorithms that assess the level of a piece of writing.  Of the multiplicity of readability measures that have been developed, I chose three that make sense for adult non-fiction writing: Flesch-Kincaid Grade Level, SMOG, and the Coleman-Liau Index.  I calculated the scores of those three measures for each Theme's blog posts (with some adjustments to the content, such as to remove lines of poems  and lyrics of songs), converted the three measures all to a level of schooling (such as 10.2 for into 10th grade), and averaged the levels.  The higher the grade level, the more complex the treatment.  This assumes that I write consistently, with a similar vocabulary breadth and style, so it is very subjective and unlikely to be included in the next round of analysis.

<!-- Could add Gunning-Fog based on article below: -->
<!-- https://medium.com/mlearning-ai/4-popular-techniques-to-measure-the-readability-of-a-text-document-32a0882db6b2 gives formulas -->

```{r readability}
# readability analyses are in BlogNLPWriting.Rmd in Themes.  Search for ~ line 1030

readable <- read_xlsx(path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/bettRwritR/ReadabilityMetrics.xlsx")

readable <- readable %>% group_by(document) %>% 
  summarise(AvgGrade = (Flesch.Kincaid + SMOG + Coleman.Liau.grade)/3)
# the empty column I had originally in the Excel worksheet threw off normalizing
readable$Theme <- str_to_lower(readable$document)
```

This metric appears in the lower right plot.\

```{r readabilityPlot, eval=TRUE}
PlotRead <- ggplot(readable, aes(x = reorder(Theme, AvgGrade), y = AvgGrade)) +
  geom_point(pch = 5, fill = "darkorange", size = 3) +
  guides(fill = "none") +
  theme_tufte(ticks = FALSE) +
  labs(y = "Average US Grade Level", x = NULL, title = "Readability", caption = "Source: Flesch-Kincaid, SMOG, Coleman-Liau") +
  theme(axis.text.x = element_text(angle = 0)) +
  scale_y_continuous(breaks = c(1:15)) +
  coord_flip()

# PlotRead

```

Here are the four preceding plots, whose metrics share an emphasis on Themes used in literary language.

```{r language, fig.width = 8, fig.height = 9}

patchAppear <- wrap_plots(PlotQuotes & theme_minimal(), PlotPoems & theme_minimal(), PlotExpress & theme_minimal(), PlotRead & theme_minimal(), ncol = 2)

patchAppear

```


## Book Titles, Count of Instances

When an author and publisher come up with a book's title, it may be an effort primarily to appeal to the market segment that is likely to buy the book (tends toward centrality) or it may be a title chosen by the author (might be complexity, e.g., "The Color Purple" or "Catcher in the Rye").

The online site [WorldCat.org](https://www.worldcat.org/advancedsearch) lets users search Format "Book", Language "English" and specify Audience "Any Audience", as well as Content "Any Content." Before running searches on Themes, I removed Dissertations (because I have a separate metric for them) and used singular versions of Theme terms, except Trains.  The results on the website appear in this format: "Results 1-10 of about 95,810".\

```{r booktitles}
books <- read_excel("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "BookTitles", range = "A1:B25")
```

The metrics collected are the number of titles found on WorldCat.org and the plot in the group of four below presents them.\

```{r booktitlesPlot, eval=TRUE}
PlotBooks <- ggplot(books, aes(x = reorder(Theme, BkTitles), y = BkTitles)) +
  geom_point(size = 3, pch = 5, color = "grey30") +
  labs(x = NULL, y = "No. of Book Titles", title = "Book Titles", caption = "\nSource: https://www.worldcat.org/advancedsearch") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0)) +
  scale_y_continuous(labels = scales::comma) +   # scientific notation
  coord_flip()

# PlotBooks
```

<!-- ## Newspapers, Count of Instances -->

<!-- An online [site](https://chicagotribune.newspapers.com/choose-a-plan/?iid=619) permits you to sign up for a month of unlimited access to 21,800+ newspapers!  -->

<!-- My original thought was to search databases of three prominent newspapers: [The New York Times](https://www.nytimes.com/search/?date_select=full&query=beauty&type=nyt&x=21&y=13), [The Washington Post](https://www.washingtonpost.com/wp-srv/world/digitalarchive/index.html), and [The Wall Street Journal](https://www.wsj.com/search?query=beauty&mod=searchresults_viewallresults). -->

<!-- The graphic below plots the data.\ -->

```{r newspapers}
papers <- read_excel("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "Newspapers", range = "A1:B25")
```


```{r papersPlot, eval=FALSE}
PlotPapers <- ggplot(papers, aes(x = reorder(Theme, NewsPapers), y = NewsPapers)) +
  geom_point(size = 3, pch = 4, color = "grey30") +
  labs(x = NULL, y = "Search Results", title = "Newspaper Frequency", caption = "\nSource -- https://chicagotribune.newspapers.com/") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0, size = 6)) +
  scale_y_continuous(labels = scales::comma) +
  coord_flip()

# PlotPapers
```


## Song Titles, Count of Instances

As with the titles of books, the choice of a title for a song may be artistic or it may be commercial: what will attract a prospective buyer of the record. Obviously, too, those who pick song titles do not want to duplicate an earlier title.  If the title caters to the pragmatic, money-making style, centrality reins; if the title derives from a less facile, more complicated source, it suggests more the complexity of the Theme.  On the whole, given the competitive pressures surrounding the efforts to produce hit songs, I favor centrality. 

To obtain a metric for how often Theme terms have appeared in the titles of songs, I searched [The Music Lyrics Database](http://www.mldb.org/).  On the header you click "Search" and then pick "title."  On July 26, 2021, MLDB.org claimed "Lyrics: 236,453, Albums: 23,932, Artists: 11,115."  After submitting a search, multiple pages of results show at the bottom of the page, but the highest number on the bottom right provides the data (I multiplied the number of total pages of results [less 1] by 30, and added the number of results on the final page).


```{r lyrics}
lyrics <- read_excel("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "SongTitles", range = "A1:B25")
```

Plotted below are the results.\

```{r lyricsPlot, eval=TRUE}
# "Some Spanish and German songs, singular and plural forms combined\nnumber of duplicates"
PlotSongs <- ggplot(lyrics, aes(x = reorder(Theme, SongTitles), y = SongTitles)) +
  geom_point(size = 3, pch = 5, color = "forestgreen") +
  labs(x = NULL, y = "No. of Titles", title = "Song Titles", caption = "\nSource: http://www.mldb.org/") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0)) +
  coord_flip()

# PlotSongs

```

## Movie Titles, Count of Instances

In the same vein as song titles, the decision to include a Theme term in a movie title might have several motivations.  But metrics based on a huge number of movie titles suggest that centrality drives more of the decisions -- the title should resonate immediately with many prospective movie-goers.  To the degree that titles of movies derive from the title of a book (or play, musical, or short story) the movie adapts, we will see overlap with book titles (or play synopses, as per below) and the choice of title is more pre-determined.

The source of these metrics is the database of more than 80,000 movies maintained by [IMDb.com](https://www.imdb.com/search/title/).  The search looked only in Titles, Feature Films (not TV, series, etc) and excluded Adult Films.

<!-- Kaggle scraped them at one point -->

```{r movies}
movies <- read_excel("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "MovieTitles", range = "A1:B25")
```

Below, in the next mosaic of four plots, is a plot of the resulting data.\

```{r moviesPlot, eval=TRUE}
PlotMovies <- ggplot(movies, aes(x = reorder(Theme, MovieTitles), y = MovieTitles)) +
  geom_point(size = 3, pch = 5) +
  labs(x = NULL, y = "No. of Movie Titles", title = "Movie Titles", caption = "\nSource:  https://www.imdb.com/search/title/") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0)) +
  coord_flip()

# PlotMovies
```

## Play Synopses, Count of Instances

To explore our hypotheses of centrality, complexity, or both, we can add synopses of plays.  Plays suggest more commonly an indicator of complexity as they deal with human psychology and emotions.  Here we deal not with titles, as in books, songs and poems, but with brief overviews of the plays.

The source for summaries of plays in English is [Play Database](http://www.playdatabase.com/) which stated in early Winter of 2021 that its "Current Totals" included 12,498 plays by 5,653 playwrites. The Navigation Bar offers a choice for "Search."  I entered each of the Theme terms into that search function and accepted all defaults. The maximum results returned is 200, so the data is truncated for the six Themes in the upper right of the graphic.\

```{r plays, message=FALSE, echo=FALSE}
plays <- read_xlsx(path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "Plays") 
plays <- plays[1:24, c("Theme", "PlaySynopsis")]
plays$Theme <- tolower(plays$Theme)  # so left_join works

```


```{r playsPlot, eval=TRUE}
PlotPlays <- ggplot(plays, aes(x = reorder(Theme, PlaySynopsis), y = PlaySynopsis)) +
  geom_point(size = 3, pch = 5, color = "blue") +
  theme_tufte(ticks = FALSE) +
  labs(y = "No. of Play Synopses", x = NULL, title = "Play Synopses", caption = "Source:  http://www.playdatabase.com/login.asp") +
  theme(axis.text.x = element_text(angle = 0)) +
  scale_y_continuous(labels = scales::comma) +
  coord_flip()

# PlotPlays

```

Here are the four preceding plots, whose metrics apply to Themes that share a source in commercial art.

```{r commercial, fig.width = 8, fig.height = 9}

patchComml <- wrap_plots(PlotBooks & theme_minimal(), PlotSongs & theme_minimal(), PlotMovies & theme_minimal(), PlotPlays & theme_minimal(), ncol = 2)

patchComml

```

## PhD Dissertations, Count of Instances

The exhaustive, original and long research that becomes a dissertation, evincing the depth, coverage, and quality that merits the award of a doctorate decree, surely suggests complexity.

For this metric, [EBSCO Open Dissertations](https://biblioboard.com/opendissertations/) has obtained from 320+ universities around the world more than 1.4 million theses and dissertations (I don't know the breakdown). For each Theme term, the EBSCO site returned results in the form of "Search Results: 1 - 10 of 3,527".  I used the final figure.

<!-- Try and compare WorldCat -->

```{r dissertations}
phds <- read_xlsx("C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "Dissertations", range = "A1:B25")
```

The quartet below displays the findings in the top left of the quadrant.\

```{r dissertationsPlot, eval=TRUE}
PlotPhDs <- ggplot(phds, aes(x = reorder(Theme, Dissertations), y = Dissertations)) +
  geom_point(size = 3, pch = 23) +
 labs(x = NULL, y = "No. of Dissertations", title = "Dissertations and Theses", caption = "\nSource:  https://biblioboardcom/opendissertations/") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0, size = 8)) +
  scale_y_continuous(labels = scales::comma) +
  coord_flip()

# PlotPhDs

```

## Code of Federal Regulations (CFR), Count of Instances

Government regulations and laws focus mostly on serious topics (by which I mean in comparison to the typically more emotional or psychological topics of songs and poems for example).  Thus, in classifying this metric, I lean toward complexity.

On the United States government's [website for the Code of Federal Regulations](https://www.ecfr.gov/search), you can query the contents all 70,000+ pages of the CFR. I searched with Find, Title	"All titles," and Date "current."  The search engine returns, for example, "Showing the most relevant 50 of 454 matching results."  The final figure became the metric for the Theme term.  The maximum values returned are 10,000 matching results (four Themes hit that ceiling).  A future extension of this project might gather comparable metrics from Eurolex, an online compendium of European Union documents.

<!-- Max 10,000 results.  I could search only certain documents to keep below the 10,000 cap.\ -->

```{r cfr}
cfr <- read_xlsx(path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "CFR", range = "A1:B25") 
```

The graph shows the results for all Themes.\

```{r cfrPlot, eval=TRUE}
PlotCFR <- ggplot(cfr, aes(x = reorder(Theme, CFRRefs), y = CFRRefs)) +
  geom_point(aes(fill = "black"), shape = 21, size = 3, color = "deeppink2") +
  guides(fill = "none") +
  labs(x = NULL, y = "Documents Found", title = "U.S. Code of Federal Regulations", caption = "\nSource: https://www.ecfr.gov/search?") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0)) +
  scale_y_continuous(labels = scales::comma) +
  coord_flip()

# PlotCFR
```

## United Nations Documents, Count of Instances

My reasoning for how to interpret documents published in the Code of Federal Regulations applies similarly to documents published by the United Nations.  That is, complex topics dominate quotidian topics. 

For this metric, I searched the UN's [Digital Library](https://digitallibrary.un.org/search?).  I chose all document types and added all the results that displayed on the left pane (it breaks the results down by several kinds of documents).

<!-- https://eur-lex.europa.eu/ -->

<!-- should I take the logs of these, since two are so out of proportion? -->

```{r un}
un <- read_xlsx(path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Metrics of Themes.xlsx", sheet = "UN", range = "A1:B25") 
```

The metric lends itself to a scatter-plot, as included below, sorted by increasing numbers of documents.

```{r unPlot, eval = TRUE}
PlotUN <- ggplot(un, aes(x = reorder(Theme, UNTotal), y = UNTotal)) +
  geom_point(shape = 21, size = 3, fill = "peachpuff2") +
  labs(x = NULL, y = "Documents Found", title = "United Nations Documents", caption = "\nSource: https://digitallibrary.un.org/search?") +
  theme_tufte(ticks = FALSE) +
  theme(axis.text.x = element_text(angle = 0)) +
  scale_y_continuous(labels = scales::comma) +
  coord_flip()

# PlotUN
```

## College Majors, Distribution

```{r majors}
# the work to isolate subthemes is in "Themes/subthemes.rmd".  It culminates in saving an Excel file, subthemesComboMajors.xlsx, where I update Majors for completed Themes. 

majors <- read_xlsx(path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/subthemesComboMajors.xlsx", sheet = "cohesion", range = "A1:E413") 

```
<!-- See "subthemes.rmd" for the calculations in subTMajorPlot and Gini coefficient -->

The 24 Themes generated `r nrow(majors)` Subthemes. I categorized each Subtheme according to which of `r length(unique(majors$Major))` college major I thought would be most likely to address it in courses. The table below lists the Themes, subthemes, major, and the blog post where they are discussed, all sorted alphabetically by major.  I regard this metric as weakly complex.

```{r table}
URLS <- read_xlsx(path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/subthemesComboMajors.xlsx", sheet = "postURLs")

cohesion <- read_xlsx(path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/subthemesComboMajors.xlsx", sheet = "cohesion")

# join by two or more variables together!
majorTable <- left_join(cohesion[  , c(1:3, 5)], URLS[  , 1:3], by = c("Theme" = "Theme", "Post" = "Post"))
majorTable$Theme <- str_to_title(majorTable$Theme)

# kableExtra::kable(majorTable[  , 1:3], longtable=TRUE) %>% 
#   kable_styling(latex_options = c("hold_position", "repeat_header"))

```

The analysis calculated for each Theme the result of dividing its number of Subthemes by its number of majors.  A Theme with seven majors and 14 Subthemes has a score of 2 (14 divided 7); a theme with six majors and 14 Subthemes has a score of 2.33. My hypothesis is that the higher the score, the more complex the Theme, since on this normalized basis, a higher score means more diverse academic areas would deem it of interest.

```{r majorsPlot, eval=TRUE, warning=FALSE, echo=FALSE}
subTMajor <- majors %>% group_by(Theme, Major) %>% summarise(Total = n())

# subTMajor.plot has same structure as subTMajor, but adds duplicated rows for the four new vars.  I want to keep the top row of each theme, and drop Major and TotalMajors

subTMajor.plot <- subTMajor %>% group_by(Theme) %>%
  mutate(TotalMajors = length(Total)) %>%    # the number of majors for a theme
  mutate(TotalThemes = sum(Total)) %>%       # the number of themes
  mutate(Gini = DescTools::Gini(Total)) %>% 
  mutate(AcadComplexity = TotalThemes/TotalMajors) %>% 
  select("Theme","TotalMajors", "TotalThemes", "Gini", "AcadComplexity") %>% 
  distinct(Theme, .keep_all = TRUE)

number_of_themes <- paste("Themes from Art blog;", sum(subTMajor.plot$TotalThemes), "subthemes")
```

After all my work to figure out the most common majors, standardize their names, classify Subthemes by those majors -- a process I did again after waiting a month, and then picking the consensus major -- I tested my process with two friends.  So many disputes arose, and so many different interpretations of the task, that I have concluded that this metric lacks credibility. It is not sufficiently empirical.  To compound the shakiness of the foundational data by calculating a complexity score makes even less sense.  Nevertheless, I have left it in for this series as a record and because with a refined methodology it might deserve inclusion as a metric.\

```{r majorsPlotTwo, eval=TRUE}
PlotMajor <- ggplot(subTMajor, aes(x = Theme, y = Total, fill = Major)) +  
  geom_col() +
  # scale_fill_viridis() +
  labs(x = NULL , y = "Majors for the 24 Themes", title = "Majors of Subthemes",  caption = "Source: Author's classification") +
  theme_tufte(ticks = FALSE) +
  theme(legend.position = "bottom", legend.title = element_blank(), legend.text = element_text(size = 5)) +
  coord_flip()

# PlotMajor
```

Here are the four preceding plots, whose metrics apply to Themes that lean toward complexity.

```{r complexity, fig.width = 8.5, fig.height = 9}

patchComplex <- wrap_plots(PlotPhDs & theme_minimal(), PlotCFR & theme_minimal(), PlotUN & theme_minimal(), PlotMajor & theme_minimal(), ncol = 2)

patchComplex

```

### Normalize the Metrics

Once all of the data described above had been collected for the 24 Themes, I created a single dataframe and converted the raw numbers of each metric into a normalized score.  This is an important step so that numbers which vary significantly in size, such as the frequency band digits of the Oxford English dictionary, which range from 0 to 8, can be compared to the number of Google search hits, which soar into the billions.  The R function scale() subtracts the average of the metric (the mean) from each value and divides that result by the *standard deviation* of the metric. By that calculation, the metrics (a.k.a. variables or features) are centered around zero and have roughly a variance of one (unit variance).

Another method of normalization converts each value into a score from 0 to 1, where the highest value would have the highest normalized score and the lowest would have the lowest normalized score -- but all of them would fall into the zero-to-one range. Another series may test this and other methods of standardizing the metrics.\

```{r combo, message=FALSE, warning=FALSE, echo=FALSE}

comboData <-   left_join(x = books, y = cfr) %>%
  left_join(., y = express[  , c("Theme", "Sayings")]) %>% 
  left_join(., y = google[  , c("Theme", "GoogleResults")]) %>%
  left_join(., y = lyrics[  , c("Theme", "SongTitles")]) %>% 
  left_join(., y = movies[  , c("Theme", "MovieTitles")]) %>%
  left_join(., y = norvig[  , c("Theme", "Trillion")]) %>%
  left_join(., y = oed[  , c("Theme", "OEDBand")]) %>% 
  left_join(., y = phds[  , c("Theme", "Dissertations")]) %>%
  left_join(., y = plays[  , c("Theme", "PlaySynopsis")]) %>% 
  left_join(., y = quotes[  , c("Theme", "TotalQuotes")]) %>% 
  left_join(., y = readable[  , c("Theme", "AvgGrade")]) %>%
  left_join(., y = poems[  , c("Theme", "PoemTotal")]) %>%  
  left_join(., y = subTMajor.plot[  , c("Theme", "AcadComplexity")]) %>% 
  left_join(., y = syns[  , c("Theme", "Synonyms")]) %>% 
  left_join(., y = top[  , c("Theme", "Top1000")]) %>% 
  left_join(., y = trends[  , c("Theme", "GSearches")]) %>% #gtrends
  left_join(., y = un[  , c("Theme", "UNTotal")]) %>% 
  left_join(., y = viewer[  , c("Theme", "Gbooks")])   #gviewer

# write_xlsx(x = comboData, path = "C:/Users/Rees Morrison/Documents/R/Projects/CLIENTS/Themes/Analytics/comboData.xlsx")
```

<!-- see https://www.pluralsight.com/guides/normalizing-data-r -->

To give a sense of the normalization, the small table below shows five Themes and three metrics, the latter in their absolute values and to the right their scaled values.

```{r normalize1, message=FALSE, warning=FALSE}
comboScaled <- scale(x = comboData[ , 2:20], center = TRUE, scale = TRUE)
# attr(comboScaled, "scaled:center") # means of each metric
# attr(comboScaled, "scaled:scale") # std dev of each metric

comboScaledDF <- as.data.frame(scale(x = comboData[ , 2:20]))
comboScaledDF$Metric <- comboData[  , 1]
# comboScaledDF[  , 1]
# var(comboScaledDF[  , 1])  # = 1

scaledTable <- bind_cols(comboData[1:5, 1:4], comboScaledDF[1:5, 1:4])
colnames(scaledTable) <- c("Theme", "Book Titles", "CFR", "Google Hits", "Book Titles Scaled", "CFR Scaled", "Google Scaled")
scaledTable <- scaledTable[  , 1:7]

kableExtra::kable(scaledTable) %>% 
     kable_styling(latex_options = "HOLD_position") 

```
