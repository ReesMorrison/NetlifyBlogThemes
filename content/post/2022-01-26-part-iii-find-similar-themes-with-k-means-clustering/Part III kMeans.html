---
title: Part III - Find Similar Themes with K-means Clustering 
author: Rees Morrison
date: '2022-01-26'
slug: []
categories:
  - Analysis
tags: ["K-means clustering",]
draft: yes
---



<p>The <a href="first">first post in this group</a> of five explains the source of the Themes written about on the blog <a href="https://themesfromart.com/">Themes from Art</a> and used in this series of posts. The <a href="second">second post</a> describes the set of 20 metrics collected for each of those Themes. This work-in-process post starts with that data set of Themes and metrics, and seeks to find most-similar Themes by <strong>k-means clustering</strong>.</p>
<!--more-->
<p>Using the vector of metrics for each Theme (the values for the 20 metrics of a Theme, treated as a sequence of numbers), the k-means algorithm identifies clusters of Themes so that within a cluster the Themes are most similar to each other, while between clusters the Themes are most dissimilar to each other. Each Theme belongs to the cluster with the nearest vector mean. Nearness is calculated based on the Euclidean distances between themes, and optimizing the squared Euclidean distances is the basis of k-means clustering. Accordingly, all the metrics must be quantitative, standardized variables (see <a href="postII">Post II</a> about standardizing).</p>
<p>The algorithm begins by me specifying <em>k</em>, the number of clusters to create. I chose five. Each of the k (5) clusters is identified as the vector of the mean value of each of the variables for the Themes within the cluster. The algorithm starts by creating at random five new points, calculates the five means (the mathematical “center” of the cluster, called the <strong>centroid</strong>), and then uses distance measures to “gravitate” each Theme to its nearest cluster mean. The means are then recalculated [the new value of a centroid is the sum of all the Themes belonging to that centroid divided by the number of Themes in the centroid’s group] and the points re-gravitate and so on until the means no longer change.</p>
<p>The sum of the squared distance between a centroid and the Theme points within its cluster constitutes the “within sum of squares due to error” (SSE) value for that cluster. The k-means algorithm minimizes the SSE by moving centroids and clustered points around to reach an optimum. When the SSE values for all the clusters are added, it becomes the “total within sum of square” value for the cluster solution. As the number of clusters increases, this value decreases.</p>
<p>According to <a href="http://varianceexplained.org/r/kmeans-free-lunch/">Variance Explained</a>, here are a few assumptions or drawbacks of the k-means algorithm:</p>
<p>*it assumes the variance of the distribution of each metric is spherical;</p>
<p>*it assumes that all the metrics have the same variance; and</p>
<p>*it assumes the prior probability for all k clusters are the same, i.e., each cluster has roughly equal numbers of Themes. What if the clusters have an uneven number of Themes – does that doom k-means clustering? In its quest to minimize the within-cluster sum of squares, the k-means algorithm gives more “weight” to larger clusters. In practice, that means it’s happy to let a small cluster end up far away from any center, while it “borrows” some from the smaller centers to “split up” a much larger cluster.</p>
<p>If any one of these three assumptions is violated, k-means will fail or not deliver reliable results.</p>
<p>*it is sensitive to outliers (unusually high or low values within a metric), and</p>
<p>*it is sensitive to the initial choice of centroids.</p>
<p>The k-means algorithm works best if the clusters are easy to tell apart, have similar volumes, and include similar numbers of objects. At this point in my effort, those desiderata are problematic.</p>
<p>Here is the plot that has created five clusters. The horizontal axis, showing “Dim1,” refers to the first <strong>principal component</strong> (to be explained in a later series, addressing an algorithm called “Principal Component Analysis”); the vertical axis, showing “Dim2”, means the second principal component. The sum of the two principal components tells approximately how much the two components explain of the original, full-dimensioned data.<br />
</p>
<p><img src="/post/2022-01-26-part-iii-find-similar-themes-with-k-means-clustering/Part%20III%20kMeans_files/figure-html/means3-1.png" width="672" /></p>
<p>For the second analysis, I asked for 12 clusters (k = 12, which is 24 Themes divided by two) to help me determine the closest pairs of Themes. To determine for each theme which other theme is closest to it, I extracted the coordinates of each Theme on the plot and then calculated the Euclidean distance between each of them to determine the closest pairs. And, here is the plot based on 12 clusters.<br />
</p>
<p><img src="/post/2022-01-26-part-iii-find-similar-themes-with-k-means-clustering/Part%20III%20kMeans_files/figure-html/clusterdivided-1.png" width="672" /></p>
